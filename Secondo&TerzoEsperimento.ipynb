{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Secondo&TerzoEsperimento.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMf46ixebA+5btQLSc1GL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pasq98/Tesi/blob/main/Secondo%26TerzoEsperimento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "g2w3ERlZCzvB"
      },
      "source": [
        "#@title Caricamento librerie\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import tensorflow\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re\n",
        "import os\n",
        "os.environ['KERAS_BACKEND']='theano'\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import initializers, optimizers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unXmdZz6C9Lf"
      },
      "source": [
        "#@title Caricamento dataset\n",
        "def load_Cybertroll(): #Carico il dataset e ritorno il testo e i label\n",
        "  CBT = pd.read_json('Dataset for Detection of Cyber-Trolls.json', lines= True)\n",
        "  CBT[\"label\"] = CBT.annotation.apply(lambda x: x.get('label'))\n",
        "  CBT[\"label\"] = CBT.label.apply(lambda x: x[0])\n",
        "  remove_n = 1\n",
        "  drop_indices = np.random.choice(CBT.index, remove_n, replace=False)\n",
        "  CBT = CBT.drop(drop_indices)\n",
        "\n",
        "  CBT = CBT.drop(CBT.query(\"label == '0'\").sample(frac=.39).index)#elimino il 39% del dataset per bilanciarlo in fase di test\n",
        "  print(CBT['label'].value_counts())\n",
        "\n",
        "  x = CBT['content']\n",
        "  label = CBT['label']\n",
        "  print(x.shape)\n",
        "  print(label.shape)\n",
        "\n",
        "  return x, label\n",
        "\n",
        "\n",
        "def load_Tweets():\n",
        "  tweets=pd.read_csv('Tweets.csv')\n",
        "  tweets_df=tweets.drop(tweets[tweets['airline_sentiment_confidence']<0.5].index,axis=0)\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@AmericanAir', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@USAirways', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@SouthwestAir', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@JetBlue', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@VirginAmerica', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@united', '')#Rimuovo hastag iniziali\n",
        "\n",
        "  #Rimuovo tweet con sentimento neutro\n",
        "  tweets_df = tweets_df.loc[tweets_df[\"airline_sentiment\"] != 'neutral']\n",
        "  tweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].str.replace('positive', '0')#Scambio etichette\n",
        "  tweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].str.replace('negative', '1')\n",
        "\n",
        "  tweets_df = tweets_df.drop(tweets_df.query(\"airline_sentiment == '1'\").sample(frac=.65).index)#elimino il 65% del dataset per bilanciarlo in fase di test\n",
        "  print(tweets_df['airline_sentiment'].value_counts())\n",
        "\n",
        "  x=tweets_df['text'] \n",
        "  label=tweets_df['airline_sentiment']\n",
        "\n",
        "  return x, label\n",
        "\n",
        "def load_sarcasm():\n",
        "  data = pd.read_csv('Sarcasm.csv', sep=';')\n",
        "  data['Column3'] =data['Column3'].str.replace('#sarcasm', '')\n",
        "  data['Column3'] = data['Column3'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
        "\n",
        "  x = data[\"Column3\"]\n",
        "  label = data['Column2']\n",
        "\n",
        "  return x,label\n",
        "\n",
        "def load_ACLL():\n",
        "  acll=pd.read_csv('ACLLTrain.csv')\n",
        "  #inverto le etichette\n",
        "  acll['Review']= acll['Review'].str.replace('1','positive') #Scambio etichette per renderle coerenti con Cybertroll\n",
        "  acll['Review']= acll['Review'].str.replace('0','negative')\n",
        "\n",
        "  acll['Review']= acll['Review'].str.replace('positive','0')\n",
        "  acll['Review']= acll['Review'].str.replace('negative','1')\n",
        "\n",
        "  x = acll['Review']\n",
        "  label = acll['Rating']\n",
        "  return x, label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "7MhdTt2ODNuT"
      },
      "source": [
        "#@title Standard preprocessing\n",
        "def standard_preprocessing(text):\n",
        "  clean_text = text.str.lower()\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  # Remove trailing spaces\n",
        "  clean_text = clean_text.str.strip()\n",
        "  # Remove Punctuations\n",
        "  clean_text = clean_text.str.replace('[^\\w\\s]',' ')\n",
        "  # Remove <br>\n",
        "  clean_text = clean_text.str.replace('br', '')\n",
        "  # Remove extra space in between words\n",
        "  clean_text = clean_text.str.replace(' +', ' ')\n",
        "  clean_text = clean_text.str.replace('\\n', ' ')# per pad sequence\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  #stop word remove\n",
        "  stop = stopwords.words('english')\n",
        "  clean_text = clean_text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop ))\n",
        "  #token\n",
        "  #clean_text = clean_text.apply(nltk.word_tokenize) #NON FARLO CON EMBEDDING\n",
        "  return clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1Hyf2aR6DXuN"
      },
      "source": [
        "#@title BLSTM model\n",
        "def blstm(inp_dim, vocab_size, embed_size,emb_matri):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_size, weights=[emb_matri] ,input_length=inp_dim, trainable=True))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Bidirectional(LSTM(50)))\n",
        "    model.add(Dropout(0.50))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    opt = optimizers.Adam(learning_rate=0.01)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'],)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "RANt_LlSDdJZ"
      },
      "source": [
        "#@title Get embedded matrix\n",
        "def get_embedded_matrix(size_vocab):\n",
        "  # create a weight matrix for words in training docs\n",
        "  embedding_matrix = np.zeros((size_vocab, 50))\n",
        "\n",
        "  for word, i in tokenizer.word_index.items():\n",
        "    try:\n",
        "      embedding_vector = glove_model.get_vector(word)\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "      embedding_vector = 0\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1Sxm-7pFM5P",
        "cellView": "form"
      },
      "source": [
        "#@title Test Cross Dataset\n",
        "#Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "################ TEST DATASET\n",
        "xTEST, labelTEST = load_Cybertroll()\n",
        "xTEST = standard_preprocessing(xTEST)\n",
        "\n",
        "#preparing vocabulary\n",
        "tokenizer.fit_on_texts(list(xTEST))\n",
        "MAX_LEN = 50\n",
        "xTEST = tokenizer.texts_to_sequences(xTEST)\n",
        "xTEST = pad_sequences(xTEST, MAX_LEN)#MAX LEN dovrebbe essere il numero di massimo di parole in una frase\n",
        "\n",
        "labelTEST = [int(lab) for lab in labelTEST] #Necessario per piu dataset\n",
        "\n",
        "size_of_vocabularyTEST=len(tokenizer.word_index) + 1 #+1 for padding\n",
        "print(\"Parole del dizionario: \",size_of_vocabularyTEST)\n",
        "\n",
        "embedding_matrixTEST = get_embedded_matrix(size_of_vocabularyTEST)\n",
        "\n",
        "#TRAIN DATASET\n",
        "xTrain, labelTrain = load_ACLL()\n",
        "xTrain = standard_preprocessing(xTrain)\n",
        "\n",
        "#preparing vocabulary\n",
        "tokenizer.fit_on_texts(list(xTrain))\n",
        "MAX_LEN = 50\n",
        "xTrain = tokenizer.texts_to_sequences(xTrain)\n",
        "xTrain = pad_sequences(xTrain, MAX_LEN)#MAX LEN dovrebbe essere il numero di massimo di parole in una frase\n",
        "\n",
        "labelTrain = [int(lab) for lab in labelTrain] #Necessario per piu dataset\n",
        "\n",
        "size_of_vocabularyTrain=len(tokenizer.word_index) + 1 #+1 for padding\n",
        "print(\"Parole del dizionario: \",size_of_vocabularyTrain)\n",
        "\n",
        "embedding_matrixTrain = get_embedded_matrix(size_of_vocabularyTrain)\n",
        "\n",
        "\n",
        "X_trainTEST, X_testTEST, y_trainTEST, y_testTEST = train_test_split(xTEST, labelTEST, test_size = 0.2, random_state = 1)\n",
        "\n",
        "#physical_devices = tensorflow.config.experimental.list_physical_devices('GPU') #Direttive per il server del dipartimento\n",
        "#tensorflow.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "\n",
        "model = blstm(MAX_LEN,size_of_vocabularyTrain,50,embedding_matrixTrain)\n",
        "model.fit(np.array(xTrain),np.array(labelTrain), epochs = 6, batch_size=64, validation_split=0.4)#val_split 0.4 for train ACLL e Tweets\n",
        "y_pred = model.predict(X_testTEST)                                                                #0.2 other\n",
        "y_pred = (y_pred > 0.5)\n",
        "report = classification_report(y_testTEST, y_pred)\n",
        "print(report)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}