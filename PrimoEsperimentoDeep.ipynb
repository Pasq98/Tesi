{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PrimoEsperimentoDeep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdlv/dH+6nG2mtLc++BaTW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pasq98/Tesi/blob/main/PrimoEsperimentoDeep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vAqb-SvT-bO",
        "outputId": "4ec8f054-11d2-4cfa-8406-25d162f65a44"
      },
      "source": [
        "#@title Librerie Deep Learning\n",
        "#Deep learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os, sys, getopt, pickle, csv, sklearn, re\n",
        "pd.set_option('display.max_columns',None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "os.environ['KERAS_BACKEND']='theano'\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Input, Concatenate\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras import initializers, optimizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul5ksLnZ7i2k"
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"/content/gensim_glove_vectors.txt\", binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "QemS1F8BUjnR"
      },
      "source": [
        "#@title Evaluate deep model\n",
        "#EVALUATE DEEP MODEL\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (classification_report,\n",
        "                             confusion_matrix,\n",
        "                             roc_auc_score)\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "  \n",
        "#report = classification_report(y_test, y_pred)\n",
        "#print(report)\n",
        "  \n",
        "def plot_cm(labels, predictions, p=0.5):\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title(\"Confusion matrix (non-normalized))\")\n",
        "    plt.ylabel(\"Actual label\")\n",
        "    plt.xlabel(\"Predicted label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT8tGRfgVA5r"
      },
      "source": [
        "#@title Load Dataset\n",
        "def load_Cybertroll(): #Carico il dataset e ritorno il testo e i label\n",
        "  CBT = pd.read_json('/content/Dataset for Detection of Cyber-Trolls.json', lines= True)\n",
        "  #np.random.seed(10)\n",
        "  #remove_n = 19500\n",
        "  #drop_indices = np.random.choice(CBT.index, remove_n, replace=False)\n",
        "  #CBT = CBT.drop(drop_indices)\n",
        "\n",
        "  CBT[\"label\"] = CBT.annotation.apply(lambda x: x.get('label'))\n",
        "  CBT[\"label\"] = CBT.label.apply(lambda x: x[0])\n",
        "\n",
        "  x = CBT['content']\n",
        "  label = CBT['label']\n",
        "\n",
        "  return x, label\n",
        "\n",
        "def load_Formsprings():\n",
        "    FS = pd.read_csv('/content/formspringPicke.csv')\n",
        "    FS = FS[FS['text'].notnull()] #delete NaN\n",
        "    #FS['text']=FS['text'].apply(str)\n",
        "    x = FS['text']\n",
        "    label = FS['label']\n",
        "    return x,label\n",
        "\n",
        "def load_Tweets(): \n",
        "  tweets=pd.read_csv('/content/Tweets.csv')\n",
        "  tweets_df=tweets.drop(tweets[tweets['airline_sentiment_confidence']<0.5].index,axis=0)\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@AmericanAir', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@USAirways', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@SouthwestAir', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@JetBlue', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@VirginAmerica', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@united', '')#Rimuovo hastag iniziali\n",
        "\n",
        "  #Rimuovo tweet con sentimento neutro\n",
        "  tweets_df = tweets_df.loc[tweets_df[\"airline_sentiment\"] != 'neutral']\n",
        "  tweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].str.replace('positive', '1')#Rimuovo hastag iniziali\n",
        "  tweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].str.replace('negative', '0')#Rimuovo hastag iniziali\n",
        "\n",
        "  x=tweets_df['text'] #testo con caratteri speciali\n",
        "  label=tweets_df['airline_sentiment']\n",
        "  return x, label\n",
        "\n",
        "def load_ACLL():\n",
        "  acll=pd.read_csv('/content/ACLLTrain.csv')\n",
        "  x = acll['Review']\n",
        "  label = acll['Rating']\n",
        "  return x, label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "9xpFmitpVbqq"
      },
      "source": [
        "#@title Standard preprocessing\n",
        "def standard_preprocessing(text):\n",
        "  clean_text = text.str.lower()\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  # Remove trailing spaces\n",
        "  clean_text = clean_text.str.strip()\n",
        "  # Remove Punctuations\n",
        "  clean_text = clean_text.str.replace('[^\\w\\s]',' ')\n",
        "  # Remove <br>\n",
        "  clean_text = clean_text.str.replace('br', '')\n",
        "  # Remove extra space in between words\n",
        "  clean_text = clean_text.str.replace(' +', ' ')\n",
        "  clean_text = clean_text.str.replace('\\n', ' ')# per pad sequence\n",
        "\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  #stop word remove \n",
        "  stop = stopwords.words('english')\n",
        "  clean_text = clean_text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop ))\n",
        "  #token\n",
        "  #clean_text = clean_text.apply(nltk.word_tokenize) #NON FARLO CON EMBEDDING\n",
        "  return clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jeOLhvuEWbf1"
      },
      "source": [
        "#@title Deep Model Embedding\n",
        "#WORK FOR FORMSPRINGS\n",
        "def blstm(inp_dim,vocab_size, embed_size, learn_rate):   \n",
        "#     K.clear_session()\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_size, weights=[embedding_matrix] ,input_length=inp_dim, trainable=True))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Bidirectional(LSTM(embed_size)))\n",
        "    model.add(Dropout(0.50))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "def cnn_model(inp_dim, vocab_size, embed_size, learn_rate):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, embed_size, weights=[embedding_matrix],input_length=inp_dim,trainable=True))\n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "#tesi DIDIO\n",
        "from keras.layers import Activation, MaxPooling1D, BatchNormalization, GRU, Bidirectional, SpatialDropout1D, Conv1D\n",
        "!pip install attention\n",
        "from attention import Attention\n",
        "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
        "def Didio(inp_dim, vocab_size, embed_size, learn_rate):\n",
        "  attention_model = Sequential()\n",
        "  attention_model.add(Embedding(vocab_size, embed_size, weights=[embedding_matrix] ,input_length=inp_dim, trainable=True))\n",
        "  attention_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "  attention_model.add(MaxPooling1D(pool_size=2))\n",
        "  attention_model.add(Dropout(0.5))\n",
        "  attention_model.add(Bidirectional(LSTM(300, return_sequences=True)))\n",
        "  attention_model.add(Attention())\n",
        "  attention_model.add(Dense(1, activation='sigmoid'))\n",
        "  nadam = optimizers.Nadam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
        "  return attention_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "DLGeXfaeXGfs"
      },
      "source": [
        "#@title Matrice di embedding\n",
        "def get_embedded_matrix(size_vocab):\n",
        "  # create a weight matrix for words in training docs\n",
        "  embedding_matrix = np.zeros((size_vocab, 50))\n",
        "\n",
        "  for word, i in tokenizer.word_index.items():\n",
        "    try:\n",
        "      embedding_vector = glove_model.get_vector(word)\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "      embedding_vector = 0\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcRZaUcOX3wL",
        "outputId": "3eec1501-c10d-499c-d7ce-02eb9ad148fb"
      },
      "source": [
        "#@title TEST DEEP LEARNING, STANDARD E EMBEDDING\n",
        "x, label = load_ACLL()\n",
        "x = standard_preprocessing(x)\n",
        "#print(label)\n",
        "\n",
        "#Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#preparing vocabulary\n",
        "tokenizer.fit_on_texts(list(x))\n",
        "MAX_LEN = 50 \n",
        "x = tokenizer.texts_to_sequences(x)\n",
        "x = pad_sequences(x, MAX_LEN)#MAX LEN dovrebbe essere il numero di massimo di parole in una frase\n",
        "\n",
        "label = [int(lab) for lab in label] #Necessario per piu dataset\n",
        "\n",
        "size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding\n",
        "print(\"Parole del dizionario: \",size_of_vocabulary)\n",
        "\n",
        "embedding_matrix = get_embedded_matrix(size_of_vocabulary)\n",
        "\n",
        "#CNN = cnn_model(MAX_LEN,size_of_vocabulary,50,0.01)\n",
        "# evaluate model with standardized dataset\n",
        "#CNN = cnn_model(MAX_LEN,size_of_vocabulary,50,0.01)\n",
        "estimator = KerasClassifier(build_fn=lambda: cnn_model(MAX_LEN,size_of_vocabulary,50,0.01), epochs=10, batch_size=5, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "results = cross_val_predict(estimator, x, label, cv=kfold)\n",
        "print(classification_report(label, results))\n",
        "#print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parole del dizionario:  73976\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "bToDYsKA0AUO"
      },
      "source": [
        "#@title DEEP MODEL TF IDF\n",
        "def blstm(X):   \n",
        "#     K.clear_session()\n",
        "    model = Sequential()  \n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Bidirectional(LSTM(200)))\n",
        "    model.add(Dropout(0.50))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "def cnn_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "#tesi DIDIO\n",
        "from keras.layers import Activation, MaxPooling1D, BatchNormalization, GRU, Bidirectional, SpatialDropout1D, Conv1D\n",
        "!pip install attention\n",
        "from attention import Attention\n",
        "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
        "def Didio():\n",
        "  attention_model = Sequential()\n",
        "  attention_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "  attention_model.add(MaxPooling1D(pool_size=2))\n",
        "  attention_model.add(Dropout(0.5))\n",
        "  attention_model.add(Bidirectional(LSTM(300, return_sequences=True)))\n",
        "  attention_model.add(Attention())\n",
        "  attention_model.add(Dense(1, activation='sigmoid'))\n",
        "  nadam = optimizers.Nadam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
        "  return attention_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "nsIZ6cVF0tDa"
      },
      "source": [
        "#@title Test TF IDF\n",
        "x, label = load_Cybertroll() #DATASET CON SOLO 1000 sample\n",
        "x = standard_preprocessing(x)#Preprocessing\n",
        "print(x.shape)\n",
        "\n",
        "#lower_sentences = [\" \".join(j) for j in x]\n",
        "\n",
        "#Applying TFIDF\n",
        "vectorizer = TfidfVectorizer(ngram_range = (1, 2))# ngram_range of (1, 1) means only unigrams,(1, 2) means unigrams and bigrams,and(2, 2) means only bigrams\n",
        "X2 = vectorizer.fit_transform(x).toarray() #shape (1001,8278)\n",
        "tf_len = len(vectorizer.vocabulary_)\n",
        "print(tf_len)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X2, label, test_size = 0.2, random_state = 1)\n",
        "#X_train shape 800,8278\n",
        "y_train = tf.strings.to_number(y_train, out_type=tf.float32)\n",
        "y_test = tf.strings.to_number(y_test, out_type=tf.float32)\n",
        "\n",
        "X_train = X_train[..., None] #(800, 8278, 1)\n",
        "\n",
        "CNN = blstm(X_train)\n",
        "CNN.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "CNN.fit(X_train, y_train, epochs= 5)\n",
        "CNN.summary()\n",
        "\n",
        "## Making predictions on our model\n",
        "prediction = CNN.predict(X_test,batch_size=1)#batch size = 1 per alcuni dataset\n",
        "y_pred = (prediction > 0.5)\n",
        "\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}