{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PrimoEsperimentoDeep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAaDFgmhkCKRZcXdff0VRq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pasq98/Tesi/blob/main/PrimoEsperimentoDeep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vAqb-SvT-bO",
        "cellView": "form"
      },
      "source": [
        "#@title Librerie Deep Learning\n",
        "#Deep learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os, sys, getopt, pickle, csv, sklearn, re\n",
        "pd.set_option('display.max_columns',None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "os.environ['KERAS_BACKEND']='theano'\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Input, Concatenate\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras import initializers, optimizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul5ksLnZ7i2k",
        "cellView": "form"
      },
      "source": [
        "#@title Load GLOVE vectors\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"/content/gensim_glove_vectors.txt\", binary=False)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "QemS1F8BUjnR"
      },
      "source": [
        "#@title Evaluate deep model\n",
        "#EVALUATE DEEP MODEL\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (classification_report,\n",
        "                             confusion_matrix,\n",
        "                             roc_auc_score)\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "  \n",
        "#report = classification_report(y_test, y_pred)\n",
        "#print(report)\n",
        "  \n",
        "def plot_cm(labels, predictions, p=0.5):\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title(\"Confusion matrix (non-normalized))\")\n",
        "    plt.ylabel(\"Actual label\")\n",
        "    plt.xlabel(\"Predicted label\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT8tGRfgVA5r",
        "cellView": "form"
      },
      "source": [
        "#@title Load Dataset\n",
        "def load_Cybertroll(): #Carico il dataset e ritorno il testo e i label\n",
        "  CBT = pd.read_json('/content/Dataset for Detection of Cyber-Trolls.json', lines= True)\n",
        "  #np.random.seed(10)\n",
        "  remove_n = 1\n",
        "  drop_indices = np.random.choice(CBT.index, remove_n, replace=False)\n",
        "  CBT = CBT.drop(drop_indices)\n",
        "  \n",
        "\n",
        "  CBT[\"label\"] = CBT.annotation.apply(lambda x: x.get('label'))\n",
        "  CBT[\"label\"] = CBT.label.apply(lambda x: x[0])\n",
        "\n",
        "  x = CBT['content']\n",
        "  label = CBT['label']\n",
        "\n",
        "  return x, label\n",
        "\n",
        "def load_Formsprings():\n",
        "    FS = pd.read_csv('/content/formspringPicke.csv')\n",
        "    FS = FS[FS['text'].notnull()] #delete NaN\n",
        "    #FS['text']=FS['text'].apply(str)\n",
        "    x = FS['text']\n",
        "    label = FS['label']\n",
        "    return x,label\n",
        "\n",
        "def load_Tweets(): \n",
        "  tweets=pd.read_csv('/content/Tweets.csv')\n",
        "  tweets_df=tweets.drop(tweets[tweets['airline_sentiment_confidence']<0.5].index,axis=0)\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@AmericanAir', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@USAirways', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@SouthwestAir', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@JetBlue', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@VirginAmerica', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@united', '')#Rimuovo hastag iniziali\n",
        "\n",
        "  #Rimuovo tweet con sentimento neutro\n",
        "  tweets_df = tweets_df.loc[tweets_df[\"airline_sentiment\"] != 'neutral']\n",
        "  tweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].str.replace('positive', '1')#Rimuovo hastag iniziali\n",
        "  tweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].str.replace('negative', '0')#Rimuovo hastag iniziali\n",
        "\n",
        "  x=tweets_df['text'] #testo con caratteri speciali\n",
        "  label=tweets_df['airline_sentiment']\n",
        "  return x, label\n",
        "\n",
        "def load_ACLL():\n",
        "  acll=pd.read_csv('/content/ACLLTrain.csv')\n",
        "  x = acll['Review']\n",
        "  label = acll['Rating']\n",
        "  return x, label\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xpFmitpVbqq",
        "cellView": "form"
      },
      "source": [
        "#@title Standard preprocessing\n",
        "def standard_preprocessing(text):\n",
        "  clean_text = text.str.lower()\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  # Remove trailing spaces\n",
        "  clean_text = clean_text.str.strip()\n",
        "  # Remove Punctuations\n",
        "  clean_text = clean_text.str.replace('[^\\w\\s]',' ')\n",
        "  # Remove <br>\n",
        "  clean_text = clean_text.str.replace('br', '')\n",
        "  # Remove extra space in between words\n",
        "  clean_text = clean_text.str.replace(' +', ' ')\n",
        "  clean_text = clean_text.str.replace('\\n', ' ')# per pad sequence\n",
        "\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  #stop word remove \n",
        "  stop = stopwords.words('english')\n",
        "  clean_text = clean_text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop ))\n",
        "  #token\n",
        "  #clean_text = clean_text.apply(nltk.word_tokenize) #NON FARLO CON EMBEDDING\n",
        "  return clean_text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeOLhvuEWbf1",
        "cellView": "form"
      },
      "source": [
        "#@title Deep Model\n",
        "\n",
        "def blstm(inp_dim,vocab_size, embed_size, emb_weight):   \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_size, weights=[emb_weight] ,input_length=inp_dim, trainable=True))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Bidirectional(LSTM(embed_size)))\n",
        "    model.add(Dropout(0.50))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "def cnn_model(inp_dim, vocab_size, embed_size, emb_weight):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, embed_size, weights=[emb_weight],input_length=inp_dim, trainable=True))\n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "from keras.layers import Activation, MaxPooling1D, BatchNormalization, GRU, Bidirectional, SpatialDropout1D, Conv1D\n",
        "!pip install attention\n",
        "from attention import Attention\n",
        "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
        "def BiLSTMAttention(inp_dim, vocab_size, embed_size, emb_weight):\n",
        "  attention_model = Sequential()\n",
        "  attention_model.add(Embedding(vocab_size, embed_size, weights=[emb_weight] ,input_length=inp_dim, trainable=True))\n",
        "  attention_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "  attention_model.add(MaxPooling1D(pool_size=2))\n",
        "  attention_model.add(Dropout(0.5))\n",
        "  attention_model.add(Bidirectional(LSTM(300, return_sequences=True)))\n",
        "  attention_model.add(Attention())\n",
        "  attention_model.add(Dense(1, activation='sigmoid'))\n",
        "  nadam = optimizers.Nadam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
        "  attention_model.compile(loss='binary_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
        "  return attention_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLGeXfaeXGfs",
        "cellView": "form"
      },
      "source": [
        "#@title Get embedding matrix\n",
        "\n",
        "def get_embedded_matrix(size_vocab):\n",
        "  # create a weight matrix for words in training docs\n",
        "  embedding_matrix = np.zeros((size_vocab, 50))\n",
        "\n",
        "  for word, i in tokenizer.word_index.items():\n",
        "    try:\n",
        "      embedding_vector = glove_model.get_vector(word)\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "      embedding_vector = 0\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcRZaUcOX3wL",
        "cellView": "form"
      },
      "source": [
        "#@title TEST DEEP LEARNING, STANDARD E EMBEDDING\n",
        "x, label = load_Cybertroll()\n",
        "x = standard_preprocessing(x)\n",
        "#print(label)\n",
        "\n",
        "#Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#preparing vocabulary\n",
        "tokenizer.fit_on_texts(list(x))\n",
        "MAX_LEN = 50 \n",
        "x = tokenizer.texts_to_sequences(x)\n",
        "x = pad_sequences(x, MAX_LEN)#MAX LEN dovrebbe essere il numero di massimo di parole in una frase\n",
        "\n",
        "label = [int(lab) for lab in label] #Necessario per piu dataset\n",
        "\n",
        "size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding\n",
        "print(\"Parole del dizionario: \",size_of_vocabulary)\n",
        "\n",
        "embedding_matrix = get_embedded_matrix(size_of_vocabulary)\n",
        "\n",
        "estimator = KerasClassifier(build_fn=lambda: Didio(MAX_LEN,size_of_vocabulary,50,0.01), epochs=10, batch_size=5, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "print(x.shape)\n",
        "results = cross_val_predict(estimator, x, label, cv=kfold, n_jobs=-1)\n",
        "print(classification_report(label, results))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "36WuZNVPNKMT"
      },
      "source": [
        "#@title Stemming\n",
        "def standardStemming(text):\n",
        "  stemmer= PorterStemmer()\n",
        "\n",
        "  clean_text = text.str.lower()\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  # Remove trailing spaces\n",
        "  clean_text = clean_text.str.strip()\n",
        "  # Remove Punctuations\n",
        "  clean_text = clean_text.str.replace('[^\\w\\s]',' ')\n",
        "  # Remove <br>\n",
        "  clean_text = clean_text.str.replace('br', '')\n",
        "  # Remove extra space in between words\n",
        "  clean_text = clean_text.str.replace(' +', ' ')\n",
        "  clean_text = clean_text.str.replace('\\n', ' ')# per pad sequence\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  #stop word remove \n",
        "  stop = stopwords.words('english')\n",
        "  clean_text = clean_text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop ))\n",
        "  #Stemming\n",
        "  clean_text = clean_text.apply(lambda x: \" \".join(stemmer.stem(x) for x in x.split() ))\n",
        " \n",
        "  return clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wze9jxfVNQ7Z",
        "cellView": "form"
      },
      "source": [
        "#@title TEST STANDARD+STEMMING+EMBEDDING\n",
        "x, label = load_ACLL()\n",
        "x = standardStemming(x)\n",
        "#print(label)\n",
        "\n",
        "#Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#preparing vocabulary\n",
        "tokenizer.fit_on_texts(list(x))\n",
        "MAX_LEN = 50 \n",
        "x = tokenizer.texts_to_sequences(x)\n",
        "x = pad_sequences(x, MAX_LEN)#MAX LEN dovrebbe essere il numero di massimo di parole in una frase\n",
        "\n",
        "label = [int(lab) for lab in label] #Necessario per piu dataset\n",
        "\n",
        "size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding\n",
        "print(\"Parole del dizionario: \",size_of_vocabulary)\n",
        "\n",
        "embedding_matrix = get_embedded_matrix(size_of_vocabulary)\n",
        "\n",
        "\n",
        "estimator = KerasClassifier(build_fn=lambda: blstm(MAX_LEN,size_of_vocabulary,50,0.01), epochs=10, batch_size=5, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "print(x.shape)\n",
        "results = cross_val_predict(estimator, x, label, cv=kfold, n_jobs=-1)\n",
        "print(classification_report(label, results))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bToDYsKA0AUO",
        "cellView": "form"
      },
      "source": [
        "#@title Deep Model TF-idf\n",
        "\n",
        "def blstm(len):   \n",
        "    model = Sequential()  \n",
        "    model.add(Embedding(input_dim=len,output_dim=300))\n",
        "    model.add(Dropout(0.55))\n",
        "    model.add(Bidirectional(LSTM(300)))\n",
        "    model.add(Dropout(0.50))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "def cnn_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "from keras.layers import Activation, MaxPooling1D, BatchNormalization, GRU, Bidirectional, SpatialDropout1D, Conv1D\n",
        "!pip install attention\n",
        "from attention import Attention\n",
        "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
        "def BiLSTMAttention(len):\n",
        "  attention_model = Sequential()\n",
        "  attention_model.add(Embedding(input_dim=len,output_dim=300))\n",
        "  attention_model.add(Bidirectional(LSTM(300, return_sequences=True)))\n",
        "  attention_model.add(Attention())\n",
        "  attention_model.add(Dense(1, activation='sigmoid'))\n",
        "  nadam = optimizers.Nadam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
        "  return attention_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsIZ6cVF0tDa",
        "cellView": "form"
      },
      "source": [
        "#@title Test TF IDF\n",
        "x, label = load_Cybertroll() \n",
        "x = standard_preprocessing(x)#Preprocessing\n",
        "print(x.shape)\n",
        "\n",
        "#lower_sentences = [\" \".join(j) for j in x]\n",
        "\n",
        "#Applying TFIDF\n",
        "vectorizer = TfidfVectorizer(ngram_range = (1, 2), max_features = 50)# ngram_range of (1, 1) means only unigrams,(1, 2) means unigrams and bigrams,and(2, 2) means only bigrams\n",
        "X = vectorizer.fit_transform(x).toarray() \n",
        "tf_len = len(vectorizer.vocabulary_)\n",
        "print(tf_len)\n",
        "#X = X[..., None]\n",
        "#label = np.array(label)\n",
        "#label = label[..., np.newaxis]\n",
        "\n",
        "## Making predictions on our model\n",
        "estimator = KerasClassifier(build_fn=lambda: blstm(tf_len), epochs=10, batch_size=5, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "results = cross_val_predict(estimator, X, label, cv=kfold)\n",
        "print(classification_report(label, results))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}