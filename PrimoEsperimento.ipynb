{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PrimoEsperimento.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTMpShS3vx3UCGn8XcQA3T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pasq98/Tesi/blob/main/PrimoEsperimento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHVfYKp5I3nD",
        "cellView": "form"
      },
      "source": [
        "#@title Libreria Shallow learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
        "\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# Import modules for evaluation purposes\n",
        "# Import libraries for predcton\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve,auc,f1_score\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNVYHp7u31Hz"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os, sys, getopt, pickle, csv, sklearn, re\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "pd.set_option('display.max_columns',None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47v6Q2Afq_vY",
        "cellView": "form"
      },
      "source": [
        "#@title Evaluate\n",
        "#EVALUATE DEEP MODEL\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (classification_report,\n",
        "                             confusion_matrix,\n",
        "                             roc_auc_score)\n",
        "\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "  \n",
        "#report = classification_report(y_test, y_pred)\n",
        "#print(report)\n",
        "  \n",
        "def plot_cm(labels, predictions, p=0.5):\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title(\"Confusion matrix (non-normalized))\")\n",
        "    plt.ylabel(\"Actual label\")\n",
        "    plt.xlabel(\"Predicted label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOnR0o386ofR",
        "cellView": "form"
      },
      "source": [
        "#@title Load Dataset\n",
        "def load_Cybertroll(): #Carico il dataset e ritorno il testo e i label\n",
        "  CBT = pd.read_json('/content/Dataset for Detection of Cyber-Trolls.json', lines= True)\n",
        "  CBT[\"label\"] = CBT.annotation.apply(lambda x: x.get('label'))\n",
        "  CBT[\"label\"] = CBT.label.apply(lambda x: x[0])\n",
        "\n",
        "  x = CBT['content']\n",
        "  label = CBT['label']\n",
        "\n",
        "  return x, label\n",
        "\n",
        "def load_Formsprings():\n",
        "    FS = pd.read_csv('/content/formspringPicke.csv')\n",
        "    FS = FS[FS['text'].notnull()] #delete NaN\n",
        "    #FS['text']=FS['text'].apply(str)\n",
        "    x = FS['text']\n",
        "    label = FS['label']\n",
        "\n",
        "    return x,label\n",
        "\n",
        "def load_Tweets(): \n",
        "  tweets=pd.read_csv('/content/Tweets.csv')\n",
        "  tweets_df=tweets.drop(tweets[tweets['airline_sentiment_confidence']<0.5].index,axis=0)\n",
        "\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@AmericanAir', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@USAirways', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@SouthwestAir', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@JetBlue', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@VirginAmerica', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@united', '')#Rimuovo hastag iniziali\n",
        "\n",
        "  #Rimuovo tweet con sentimento neutro\n",
        "  tweets_df = tweets_df.loc[tweets_df[\"airline_sentiment\"] != 'neutral']\n",
        "\n",
        "  tweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].str.replace('positive', '1')\n",
        "  tweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].str.replace('negative', '0')\n",
        "\n",
        "  x=tweets_df['text'] #testo con caratteri speciali\n",
        "  label=tweets_df['airline_sentiment']\n",
        "  return x, label\n",
        "\n",
        "def load_ACLL():\n",
        "  acll=pd.read_csv('/content/ACLLTrain.csv')\n",
        "  x = acll['Review']\n",
        "  label = acll['Rating']\n",
        "  return x, label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPE8p26TtRyj",
        "cellView": "form"
      },
      "source": [
        "#@title Preprocessing\n",
        "#WORK FOR ALL DATASET \n",
        "#ADD REMOVE STOPWORD\n",
        "def standard_preprocessing(text):\n",
        "  clean_text = text.str.lower()\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  # Remove trailing spaces\n",
        "  clean_text = clean_text.str.strip()\n",
        "  # Remove Punctuations\n",
        "  clean_text = clean_text.str.replace('[^\\w\\s]',' ')\n",
        "  # Remove <br>\n",
        "  clean_text = clean_text.str.replace('br', '')\n",
        "  # Remove extra space in between words\n",
        "  clean_text = clean_text.str.replace(' +', ' ')\n",
        "  clean_text = clean_text.str.replace('\\n', ' ')# per pad sequence\n",
        "\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  #stop word remove \n",
        "  stop = stopwords.words('english')\n",
        "  clean_text = clean_text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop ))\n",
        "  #token\n",
        "  clean_text = clean_text.apply(nltk.word_tokenize)# Per il deep provo a farlo in un altro modo\n",
        "  return clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snAQC4nFAU6s"
      },
      "source": [
        "#@title Embedding\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"/content/gensim_glove_vectors.txt\", binary=False)\n",
        "\n",
        "def glove_embedding(text):\n",
        "  v = glove_model.get_vector('king')\n",
        "  D = v.shape[0]\n",
        "  emptycount = 0\n",
        "  X = np.zeros((len(text), D))\n",
        "  n = 0\n",
        "  for sentence in text:  \n",
        "    vecs = []\n",
        "    for word in sentence:\n",
        "      try:\n",
        "        # throws KeyError if word not found\n",
        "        vec = glove_model.get_vector(word)\n",
        "        vecs.append(vec)\n",
        "      except KeyError:\n",
        "        pass\n",
        "    if len(vecs) > 0:\n",
        "      vecs = np.array(vecs)\n",
        "      X[n] = vecs.mean(axis=0)\n",
        "    else:\n",
        "      emptycount += 1\n",
        "    n += 1\n",
        "  print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(text)))\n",
        "  return X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSM4psgcFN19"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def RF_grid(X,y):\n",
        "  # Number of trees in random forest\n",
        "  n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "  # Number of features to consider at every split\n",
        "  max_features = ['auto', 'sqrt']\n",
        "  # Maximum number of levels in tree\n",
        "  max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "  max_depth.append(None)\n",
        "  # Minimum number of samples required to split a node\n",
        "  min_samples_split = [2, 5, 10]\n",
        "  # Minimum number of samples required at each leaf node\n",
        "  min_samples_leaf = [1, 2, 4]\n",
        "  # Method of selecting samples for training each tree\n",
        "  bootstrap = [True, False]\n",
        "  # Create the random grid\n",
        "  random_grid = {'n_estimators': n_estimators,\n",
        "                'max_features': max_features,\n",
        "                'max_depth': max_depth,\n",
        "                'min_samples_split': min_samples_split,\n",
        "                'min_samples_leaf': min_samples_leaf,\n",
        "                'bootstrap': bootstrap}\n",
        "  #print(random_grid)\n",
        "\n",
        "  # Use the random grid to search for best hyperparameters\n",
        "  # First create the base model to tune\n",
        "  rf = RandomForestClassifier()\n",
        "  # Random search of parameters, using 3 fold cross validation, \n",
        "  # search across 100 different combinations, and use all available cores\n",
        "  rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "  rf_random.fit(X_train,y_train)\n",
        "\n",
        "  return rf_random.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ygK04tvWQ-"
      },
      "source": [
        "#TEST STANDARD PREPREPROCESSING E EMBEDDING TRADITIONAL ML\n",
        "x, label = load_Formsprings()\n",
        "x = standard_preprocessing(x)#Preprocessing\n",
        "x = glove_embedding(x) #Feature extraction\n",
        "#print(x)\n",
        "print(x.dtype)\n",
        "print(label.dtype)\n",
        "label = [int(lab) for lab in label]\n",
        "\n",
        "randomForest = RandomForestClassifier()\n",
        "SVM = svm.SVC(kernel='linear')\n",
        "LR = LogisticRegression(max_iter=500, class_weight='balanced')\n",
        "\n",
        "\n",
        "#model.fit(X_train, y_train)\n",
        "#y_pred = model.predict(X_test)\n",
        "#print(classification_report(y_test, y_pred))\n",
        "\n",
        "y_pred = cross_val_predict(SVM,x,label, cv=5)\n",
        "print(classification_report(label, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej8tps3113l0"
      },
      "source": [
        "#@title GET TF IDF\n",
        "def get_tf_idf(text):\n",
        "  # Applying TFIDF\n",
        "  # You can still get n-grams here\n",
        "  lower_sentences = [\" \".join(j) for j in text]\n",
        "  vectorizer = TfidfVectorizer(ngram_range = (1, 2))# ngram_range of (1, 1) means only unigrams,(1, 2) means unigrams and bigrams,and(2, 2) means only bigrams\n",
        "  tf = vectorizer.fit_transform(lower_sentences)\n",
        "  return tf"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}