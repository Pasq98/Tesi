{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PrimoEsperimento.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGvGtOM5yC5oy00uBBqYJg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pasq98/Tesi/blob/main/PrimoEsperimento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHVfYKp5I3nD",
        "cellView": "form"
      },
      "source": [
        "#@title Libreria Shallow learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# Import modules for evaluation purposes\n",
        "# Import libraries for predcton\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve,auc,f1_score\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os, sys, getopt, pickle, csv, sklearn, re\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47v6Q2Afq_vY",
        "cellView": "form"
      },
      "source": [
        "#@title Evaluate\n",
        "#Funzione per valutare le prestazione di un classificatore per ogni classe\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (classification_report,\n",
        "                             confusion_matrix,\n",
        "                             roc_auc_score)\n",
        "\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "  \n",
        "\n",
        "  \n",
        "def plot_cm(labels, predictions, p=0.5):\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title(\"Confusion matrix (non-normalized))\")\n",
        "    plt.ylabel(\"Actual label\")\n",
        "    plt.xlabel(\"Predicted label\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOnR0o386ofR"
      },
      "source": [
        "#@title Load Dataset\n",
        "def load_Cybertroll(): #Carico il dataset e ritorno il testo e i label\n",
        "  CBT = pd.read_json('/content/Dataset for Detection of Cyber-Trolls.json', lines= True)\n",
        "  CBT[\"label\"] = CBT.annotation.apply(lambda x: x.get('label'))\n",
        "  CBT[\"label\"] = CBT.label.apply(lambda x: x[0])\n",
        "\n",
        "  x = CBT['content']\n",
        "  label = CBT['label']\n",
        "\n",
        "  return x, label\n",
        "\n",
        "def load_Formsprings():\n",
        "    FS = pd.read_csv('/content/formspringPicke.csv')\n",
        "    FS = FS[FS['text'].notnull()] #delete NaN\n",
        "    #FS['text']=FS['text'].apply(str)\n",
        "    x = FS['text']\n",
        "    label = FS['label']\n",
        "\n",
        "    return x,label\n",
        "\n",
        "def load_Tweets(): \n",
        "  tweets=pd.read_csv('/content/Tweets.csv')\n",
        "  tweets_df=tweets.drop(tweets[tweets['airline_sentiment_confidence']<0.5].index,axis=0)\n",
        "\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@AmericanAir', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@USAirways', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@SouthwestAir', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@JetBlue', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@VirginAmerica', '')#Rimuovo hastag iniziali\n",
        "  tweets_df['text'] = tweets_df['text'].str.replace('@united', '')#Rimuovo hastag iniziali\n",
        "\n",
        "  #Rimuovo tweet con sentimento neutro\n",
        "  tweets_df = tweets_df.loc[tweets_df[\"airline_sentiment\"] != 'neutral']\n",
        "\n",
        "  tweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].str.replace('positive', '1')\n",
        "  tweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].str.replace('negative', '0')\n",
        "\n",
        "  x=tweets_df['text'] #testo con caratteri speciali\n",
        "  label=tweets_df['airline_sentiment']\n",
        "  return x, label\n",
        "\n",
        "def load_ACLL():\n",
        "  acll=pd.read_csv('/content/ACLLTrain.csv')\n",
        "  x = acll['Review']\n",
        "  label = acll['Rating']\n",
        "  return x, label\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPE8p26TtRyj"
      },
      "source": [
        "#@title Preprocessing Standard\n",
        "def standard_preprocessing(text):\n",
        "  clean_text = text.str.lower()\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  # Remove trailing spaces\n",
        "  clean_text = clean_text.str.strip()\n",
        "  # Remove Punctuations\n",
        "  clean_text = clean_text.str.replace('[^\\w\\s]',' ')\n",
        "  # Remove <br>\n",
        "  clean_text = clean_text.str.replace('br', '')\n",
        "  # Remove extra space in between words\n",
        "  clean_text = clean_text.str.replace(' +', ' ')\n",
        "  clean_text = clean_text.str.replace('\\n', ' ')# per pad sequence\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  #stop word remove \n",
        "  stop = stopwords.words('english')\n",
        "  clean_text = clean_text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop ))\n",
        "  #token\n",
        "  clean_text = clean_text.apply(nltk.word_tokenize)# Per il deep provo a farlo in un altro modo\n",
        "  return clean_text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVriSmERCsbJ",
        "cellView": "form"
      },
      "source": [
        "#@title Standard con Stemming\n",
        "def standardStemming(text):\n",
        "  stemmer= PorterStemmer()\n",
        "\n",
        "  clean_text = text.str.lower()\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  # Remove trailing spaces\n",
        "  clean_text = clean_text.str.strip()\n",
        "  # Remove Punctuations\n",
        "  clean_text = clean_text.str.replace('[^\\w\\s]',' ')\n",
        "  # Remove <br>\n",
        "  clean_text = clean_text.str.replace('br', '')\n",
        "  # Remove extra space in between words\n",
        "  clean_text = clean_text.str.replace(' +', ' ')\n",
        "  clean_text = clean_text.str.replace('\\n', ' ')# per pad sequence\n",
        "  # Remove Numbers\n",
        "  clean_text = clean_text.str.replace('\\d+', '')\n",
        "  #stop word remove \n",
        "  stop = stopwords.words('english')\n",
        "  clean_text = clean_text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop ))\n",
        "  #Stemming\n",
        "  clean_text = clean_text.apply(lambda x: \" \".join(stemmer.stem(x) for x in x.split() ))\n",
        "  #token\n",
        "  clean_text = clean_text.apply(nltk.word_tokenize)# Per il deep provo a farlo in un altro modo\n",
        "  return clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snAQC4nFAU6s"
      },
      "source": [
        "#@title Embedding\n",
        "#Load glove vector\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"/content/gensim_glove_vectors.txt\", binary=False)\n",
        "\n",
        "def glove_embedding(text):\n",
        "  v = glove_model.get_vector('king')\n",
        "  D = v.shape[0]\n",
        "  emptycount = 0\n",
        "  X = np.zeros((len(text), D))\n",
        "  n = 0\n",
        "  for sentence in text:  \n",
        "    vecs = []\n",
        "    for word in sentence:\n",
        "      try:\n",
        "        # throws KeyError if word not found\n",
        "        vec = glove_model.get_vector(word)\n",
        "        vecs.append(vec)\n",
        "      except KeyError:\n",
        "        pass\n",
        "    if len(vecs) > 0:\n",
        "      vecs = np.array(vecs)\n",
        "      X[n] = vecs.mean(axis=0)\n",
        "    else:\n",
        "      emptycount += 1\n",
        "    n += 1\n",
        "  print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(text)))\n",
        "  return X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ygK04tvWQ-"
      },
      "source": [
        "#@title Test STANDARD+EMBEDDING\n",
        "#TEST STANDARD PREPREPROCESSING E EMBEDDING TRADITIONAL ML\n",
        "x, label = load_ACLL()\n",
        "x = standard_preprocessing(x)#Preprocessing\n",
        "x = glove_embedding(x) #Feature extraction\n",
        "#print(x)\n",
        "print(x.dtype)\n",
        "print(label.dtype)\n",
        "label = [int(lab) for lab in label]\n",
        "\n",
        "randomForest = RandomForestClassifier()\n",
        "SVM = svm.SVC(kernel='linear')\n",
        "LR = LogisticRegression(max_iter=500, class_weight='balanced')\n",
        "\n",
        "y_pred = cross_val_predict(LR,x,label, cv=5, n_jobs=-1)\n",
        "print(classification_report(label, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfq8athsDmux",
        "cellView": "form"
      },
      "source": [
        "#@title Test STANDARD+STEMMING+EMBEDDING\n",
        "x, label = load_ACLL()\n",
        "x = standardStemming(x)#Preprocessing con stemming\n",
        "x = glove_embedding(x) #Feature extraction\n",
        "\n",
        "label = [int(lab) for lab in label]\n",
        "\n",
        "randomForest = RandomForestClassifier()\n",
        "SVM = svm.SVC(kernel='linear')\n",
        "LR = LogisticRegression(max_iter=500, class_weight='balanced')\n",
        "\n",
        "\n",
        "#model.fit(X_train, y_train)\n",
        "#y_pred = model.predict(X_test)\n",
        "#print(classification_report(y_test, y_pred))\n",
        "\n",
        "y_pred = cross_val_predict(LR,x,label, cv=5, n_jobs=-1)\n",
        "print(classification_report(label, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej8tps3113l0",
        "cellView": "form"
      },
      "source": [
        "#@title get tf-idf\n",
        "def get_tf_idf(text):\n",
        "  # Applying TFIDF\n",
        "  # You can still get n-grams here\n",
        "  lower_sentences = [\" \".join(j) for j in text]\n",
        "  vectorizer = TfidfVectorizer(ngram_range = (1, 2),max_features=500)# ngram_range of (1, 1) means only unigrams,(1, 2) means unigrams and bigrams,and(2, 2) means only bigrams\n",
        "  tf = vectorizer.fit_transform(lower_sentences)\n",
        "  return tf"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxlc3pDgOET5",
        "cellView": "form"
      },
      "source": [
        "#@title STANDARD+TF IDF\n",
        "x, label = load_Cybertroll()\n",
        "x = standard_preprocessing(x)#Preprocessing con stemming\n",
        "x = get_tf_idf(x)\n",
        "label = [int(lab) for lab in label]\n",
        "\n",
        "randomForest = RandomForestClassifier()\n",
        "SVM = svm.SVC(kernel='linear')\n",
        "LR = LogisticRegression(max_iter=500, class_weight='balanced')\n",
        "\n",
        "y_pred = cross_val_predict(LR,x,label, cv=5, n_jobs=-1)\n",
        "print(classification_report(label, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}